{
  "cells": [
    {
      "cell_type": "code",
      "source": "import torch\nimport cv2",
      "metadata": {
        "tags": [],
        "cell_id": "00000-12501a5d-d2e6-4dd5-8c8f-fd69bfe1f078",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "98ec7abe",
        "execution_millis": 2,
        "execution_start": 1616668275126,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "#!apt update\n#!apt install ffmpeg libsm6 libxext6 -y",
      "metadata": {
        "tags": [],
        "cell_id": "00001-92a53a4a-7b14-4b99-8c0a-8f7315f3a74a",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e21bc034",
        "execution_millis": 4161,
        "execution_start": 1616667728406,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Hit:1 http://deb.debian.org/debian buster InRelease\nHit:2 http://deb.debian.org/debian buster-updates InRelease\nHit:3 http://security.debian.org/debian-security buster/updates InRelease\n\n\n\n1 package can be upgraded. Run 'apt list --upgradable' to see it.\n\n\n\nffmpeg is already the newest version (7:4.1.6-1~deb10u1).\nlibsm6 is already the newest version (2:1.2.3-1).\nlibxext6 is already the newest version (2:1.3.3-1+b2).\n0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-f4e39e08-7f2a-43fa-aa05-8d308a4c2914",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5a04a685",
        "execution_millis": 4336,
        "execution_start": 1616668468353,
        "deepnote_cell_type": "code"
      },
      "source": "# Model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n\n# Images\ndir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/'\nimgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batched list of images\n\n# Inference\nresults = model(imgs)\nprint(\"results are: \", results)\nresults.save() # show() #.print()  # or .show(), .save()",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stderr",
          "text": "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nModel Summary: 283 layers, 7276605 parameters, 7276605 gradients\n\nYOLOv5 ðŸš€ 68fd658 torch 1.8.0 CPU\n\nAdding autoShape... \nresults are:  <models.common.Detections object at 0x7f675dddcfd0>\nSaving results/zidane.jpg, results/bus.jpg, done.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "results.pred",
      "metadata": {
        "tags": [],
        "cell_id": "00002-4aca2880-041d-43ab-a935-24e7b7467a62",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7bcd8554",
        "execution_millis": 6,
        "execution_start": 1616668083381,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "[tensor([[7.50637e+02, 4.37278e+01, 1.15887e+03, 7.08682e+02, 8.18137e-01, 0.00000e+00],\n         [9.33599e+01, 2.07387e+02, 1.04737e+03, 7.10224e+02, 5.78011e-01, 0.00000e+00],\n         [4.24503e+02, 4.29092e+02, 5.16300e+02, 7.16425e+02, 5.68713e-01, 2.70000e+01]]),\n tensor([[5.70690e+01, 3.91771e+02, 2.41384e+02, 9.05798e+02, 8.68964e-01, 0.00000e+00],\n         [6.67661e+02, 3.99303e+02, 8.10000e+02, 8.81397e+02, 8.51887e-01, 0.00000e+00],\n         [2.22878e+02, 4.14774e+02, 3.43804e+02, 8.57825e+02, 8.38376e-01, 0.00000e+00],\n         [4.20533e+00, 2.34448e+02, 8.03739e+02, 7.50023e+02, 6.58006e-01, 5.00000e+00],\n         [0.00000e+00, 5.50596e+02, 7.66812e+01, 8.78670e+02, 4.50596e-01, 0.00000e+00]])]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "video = cv2.VideoCapture('street.mp4')\n\nframe_count = 0 \nsaved = 0 \n\nwhile True:\n    frame_count += 1\n    ok, frame = video.read()\n    if ok: \n        #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        if frame_count % 150 == 0:\n            res = model(frame)\n            cords = res.xyxy\n            crop_person(cords, frame)\n            #name = \"results/\" + str(frame_count) + \"img\"\n            #res.save(name)\n    else: \n        break ",
      "metadata": {
        "tags": [],
        "cell_id": "00004-e40cbc58-cb6d-4a80-9140-a5c0826f3563",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e7b20843",
        "execution_millis": 8058,
        "execution_start": 1616672144087,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "source": "def crop_person(tensor_array, img):\n    i = -1\n    for box in tensor_array: \n        for b in box:\n            i += 1 \n            if b[-1:] == 0.0:\n                #print(\"person\")\n                x1 = int(b[0:1].item())\n                y1 = int(b[1:2].item())\n                x2  = int(b[2:3].item())\n                y2 = int(b[3:4].item())\n                #print(x1,y1,w,h)\n                c_img = img[y1:y2, x1:x2]\n                name = \"results/Crops/Person\" + str(i) + \".png\"\n                cv2.imwrite(name, c_img)\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "00005-ee370991-e14e-4853-8f31-fa1053e423e0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1131a109",
        "execution_millis": 2,
        "execution_start": 1616671856551,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 41
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00007-0d4c2322-3488-4545-ab71-16b8c019e780",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=9f1e7bdb-154a-4549-a646-7775bf1dc7a2' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "2c931a58-d288-48c5-a99c-dde5be22b340",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}
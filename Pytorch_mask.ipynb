{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:44:06.641517Z",
          "start_time": "2021-03-24T09:44:06.120203Z"
        },
        "cell_id": "00000-4261aa2f-7465-445c-abda-36d2791037bc",
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn as nn\nimport torch.functional as F\nimport torch.optim as optim\nfrom torchvision import models\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom collections import OrderedDict",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:44:07.138827Z",
          "start_time": "2021-03-24T09:44:07.127985Z"
        },
        "cell_id": "00001-ec60b0c5-32df-459d-aaf0-35f6adc77d47",
        "deepnote_cell_type": "code"
      },
      "source": "#should output the images in 640 x 640\ndef load_data(data_folder, batch_size, train):\n    transform = {\n        'train': transforms.Compose(\n            [transforms.Resize([224, 224]),\n                #transforms.RandomCrop(224),\n                #transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                  std=[0.229, 0.224, 0.225])]),\n        'test': transforms.Compose(\n            [transforms.Resize([224, 224]),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                  std=[0.229, 0.224, 0.225])])\n        }\n    data = datasets.ImageFolder(root = data_folder, transform=transform['train' if train else 'test'])\n    total_count = len(data)\n    # Also you shouldn't use transforms here but below\n    train_count = int(0.7 * total_count)\n    valid_count = int(0.2 * total_count)\n    test_count = total_count - train_count - valid_count\n    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(data, (train_count, valid_count, test_count))\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True if train else False)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last = True if train else False)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last = True if train else False)\n    \n    return train_loader, valid_loader, test_loader",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:44:08.921193Z",
          "start_time": "2021-03-24T09:44:08.913263Z"
        },
        "cell_id": "00002-5bb87e25-055c-442c-8d91-793c3a6c70f7",
        "deepnote_cell_type": "code"
      },
      "source": "train_loader, valid_loader, test_loader = load_data('dataset', 16, True)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:44:10.562308Z",
          "start_time": "2021-03-24T09:44:09.992139Z"
        },
        "cell_id": "00003-171611e0-a08e-4cf4-b4f8-5ef929daaea5",
        "deepnote_cell_type": "code"
      },
      "source": "model = models.resnet18(pretrained=True)\nmodel",
      "outputs": [
        {
          "data": {
            "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:44:17.654468Z",
          "start_time": "2021-03-24T09:44:17.648484Z"
        },
        "cell_id": "00004-be3b3657-dc2e-4e7a-9ad6-fa150a716782",
        "deepnote_cell_type": "code"
      },
      "source": "for param in model.parameters():\n    param.requires_grad = False\n\n\nclassifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(512, 100)),\n                          ('relu', nn.ReLU()),\n                          ('fc2', nn.Linear(100, 5)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n    \nmodel.fc = classifier",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:44:19.557328Z",
          "start_time": "2021-03-24T09:44:19.553285Z"
        },
        "cell_id": "00005-d67f7f5b-f37a-4a53-ab86-3636153ceecc",
        "deepnote_cell_type": "code"
      },
      "source": "\ncriterion = nn.NLLLoss()\n\noptimizer = optim.SGD(model.fc.parameters() , lr=0.001, momentum=0.9)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:44:20.651263Z",
          "start_time": "2021-03-24T09:44:20.647951Z"
        },
        "cell_id": "00006-01cea011-b7cb-47ad-b02d-2409d054cd9e",
        "deepnote_cell_type": "code"
      },
      "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T10:01:14.777771Z",
          "start_time": "2021-03-24T09:44:21.704030Z"
        },
        "cell_id": "00007-37248fa6-8555-415d-a765-bc959bd3a1c4",
        "deepnote_cell_type": "code"
      },
      "source": "epochs = 5\nsteps = 0\nrunning_loss = 0\nprint_every = 1\nmax_accuracy = 0\nfor epoch in range(epochs):\n    for inputs, labels in train_loader:\n        \n        steps += 1\n        # Move input and label tensors to the default device\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        logps = model.forward(inputs)\n        loss = criterion(logps, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        \n        if steps % print_every == 0:\n            test_loss = 0\n            accuracy = 0\n            model.eval()\n            with torch.no_grad():\n                for inputs, labels in valid_loader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    logps = model.forward(inputs)\n                    batch_loss = criterion(logps, labels)\n                    \n                    test_loss += batch_loss.item()\n                    \n                    # Calculate accuracy\n                    ps = torch.exp(logps)\n                    top_p, top_class = ps.topk(1, dim=1)\n                    equals = top_class == labels.view(*top_class.shape)\n                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n                    if accuracy >= max_accuracy:\n                        max_accuracy = accuracy\n                        torch.save(model.state_dict(), 'checkpoint.pth')\n            print(f\"Epoch {epoch+1}/{epochs}.. \"\n                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n                  f\"Test loss: {test_loss/len(valid_loader):.3f}.. \"\n                  f\"Test accuracy: {accuracy/len(valid_loader):.3f}\")\n            running_loss = 0\n            model.train()\n    ",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/5.. Train loss: 1.609.. Test loss: 1.614.. Test accuracy: 0.263\nEpoch 1/5.. Train loss: 1.678.. Test loss: 1.612.. Test accuracy: 0.263\nEpoch 1/5.. Train loss: 1.622.. Test loss: 1.602.. Test accuracy: 0.300\nEpoch 1/5.. Train loss: 1.628.. Test loss: 1.599.. Test accuracy: 0.287\nEpoch 1/5.. Train loss: 1.599.. Test loss: 1.599.. Test accuracy: 0.312\nEpoch 1/5.. Train loss: 1.624.. Test loss: 1.590.. Test accuracy: 0.312\nEpoch 1/5.. Train loss: 1.555.. Test loss: 1.576.. Test accuracy: 0.325\nEpoch 1/5.. Train loss: 1.590.. Test loss: 1.570.. Test accuracy: 0.325\nEpoch 1/5.. Train loss: 1.569.. Test loss: 1.565.. Test accuracy: 0.412\nEpoch 1/5.. Train loss: 1.585.. Test loss: 1.551.. Test accuracy: 0.475\nEpoch 1/5.. Train loss: 1.599.. Test loss: 1.553.. Test accuracy: 0.487\nEpoch 1/5.. Train loss: 1.577.. Test loss: 1.547.. Test accuracy: 0.450\nEpoch 1/5.. Train loss: 1.522.. Test loss: 1.543.. Test accuracy: 0.463\nEpoch 1/5.. Train loss: 1.527.. Test loss: 1.544.. Test accuracy: 0.450\nEpoch 1/5.. Train loss: 1.575.. Test loss: 1.529.. Test accuracy: 0.475\nEpoch 1/5.. Train loss: 1.499.. Test loss: 1.525.. Test accuracy: 0.438\nEpoch 1/5.. Train loss: 1.483.. Test loss: 1.521.. Test accuracy: 0.312\nEpoch 1/5.. Train loss: 1.631.. Test loss: 1.522.. Test accuracy: 0.287\nEpoch 2/5.. Train loss: 1.599.. Test loss: 1.529.. Test accuracy: 0.263\nEpoch 2/5.. Train loss: 1.475.. Test loss: 1.530.. Test accuracy: 0.263\nEpoch 2/5.. Train loss: 1.639.. Test loss: 1.508.. Test accuracy: 0.300\nEpoch 2/5.. Train loss: 1.438.. Test loss: 1.519.. Test accuracy: 0.250\nEpoch 2/5.. Train loss: 1.553.. Test loss: 1.506.. Test accuracy: 0.275\nEpoch 2/5.. Train loss: 1.475.. Test loss: 1.515.. Test accuracy: 0.275\nEpoch 2/5.. Train loss: 1.465.. Test loss: 1.504.. Test accuracy: 0.300\nEpoch 2/5.. Train loss: 1.441.. Test loss: 1.491.. Test accuracy: 0.300\nEpoch 2/5.. Train loss: 1.549.. Test loss: 1.505.. Test accuracy: 0.263\nEpoch 2/5.. Train loss: 1.451.. Test loss: 1.471.. Test accuracy: 0.287\nEpoch 2/5.. Train loss: 1.456.. Test loss: 1.479.. Test accuracy: 0.275\nEpoch 2/5.. Train loss: 1.473.. Test loss: 1.475.. Test accuracy: 0.287\nEpoch 2/5.. Train loss: 1.515.. Test loss: 1.470.. Test accuracy: 0.275\nEpoch 2/5.. Train loss: 1.486.. Test loss: 1.464.. Test accuracy: 0.287\nEpoch 2/5.. Train loss: 1.414.. Test loss: 1.471.. Test accuracy: 0.275\nEpoch 2/5.. Train loss: 1.467.. Test loss: 1.453.. Test accuracy: 0.300\nEpoch 2/5.. Train loss: 1.447.. Test loss: 1.452.. Test accuracy: 0.312\nEpoch 2/5.. Train loss: 1.575.. Test loss: 1.467.. Test accuracy: 0.300\nEpoch 3/5.. Train loss: 1.388.. Test loss: 1.440.. Test accuracy: 0.362\nEpoch 3/5.. Train loss: 1.450.. Test loss: 1.453.. Test accuracy: 0.312\nEpoch 3/5.. Train loss: 1.449.. Test loss: 1.442.. Test accuracy: 0.362\nEpoch 3/5.. Train loss: 1.538.. Test loss: 1.434.. Test accuracy: 0.388\nEpoch 3/5.. Train loss: 1.395.. Test loss: 1.420.. Test accuracy: 0.475\nEpoch 3/5.. Train loss: 1.589.. Test loss: 1.410.. Test accuracy: 0.487\nEpoch 3/5.. Train loss: 1.362.. Test loss: 1.395.. Test accuracy: 0.550\nEpoch 3/5.. Train loss: 1.400.. Test loss: 1.395.. Test accuracy: 0.550\nEpoch 3/5.. Train loss: 1.413.. Test loss: 1.373.. Test accuracy: 0.575\nEpoch 3/5.. Train loss: 1.371.. Test loss: 1.370.. Test accuracy: 0.575\nEpoch 3/5.. Train loss: 1.425.. Test loss: 1.382.. Test accuracy: 0.550\nEpoch 3/5.. Train loss: 1.381.. Test loss: 1.376.. Test accuracy: 0.550\nEpoch 3/5.. Train loss: 1.508.. Test loss: 1.379.. Test accuracy: 0.525\nEpoch 3/5.. Train loss: 1.478.. Test loss: 1.368.. Test accuracy: 0.525\nEpoch 3/5.. Train loss: 1.436.. Test loss: 1.343.. Test accuracy: 0.575\nEpoch 3/5.. Train loss: 1.392.. Test loss: 1.360.. Test accuracy: 0.512\nEpoch 3/5.. Train loss: 1.376.. Test loss: 1.348.. Test accuracy: 0.562\nEpoch 3/5.. Train loss: 1.454.. Test loss: 1.321.. Test accuracy: 0.588\nEpoch 4/5.. Train loss: 1.390.. Test loss: 1.340.. Test accuracy: 0.562\nEpoch 4/5.. Train loss: 1.338.. Test loss: 1.341.. Test accuracy: 0.550\nEpoch 4/5.. Train loss: 1.365.. Test loss: 1.335.. Test accuracy: 0.575\nEpoch 4/5.. Train loss: 1.312.. Test loss: 1.328.. Test accuracy: 0.575\nEpoch 4/5.. Train loss: 1.350.. Test loss: 1.330.. Test accuracy: 0.550\nEpoch 4/5.. Train loss: 1.403.. Test loss: 1.314.. Test accuracy: 0.562\nEpoch 4/5.. Train loss: 1.417.. Test loss: 1.328.. Test accuracy: 0.550\nEpoch 4/5.. Train loss: 1.328.. Test loss: 1.308.. Test accuracy: 0.562\nEpoch 4/5.. Train loss: 1.376.. Test loss: 1.303.. Test accuracy: 0.600\nEpoch 4/5.. Train loss: 1.275.. Test loss: 1.287.. Test accuracy: 0.625\nEpoch 4/5.. Train loss: 1.304.. Test loss: 1.287.. Test accuracy: 0.600\nEpoch 4/5.. Train loss: 1.395.. Test loss: 1.274.. Test accuracy: 0.613\nEpoch 4/5.. Train loss: 1.449.. Test loss: 1.279.. Test accuracy: 0.613\nEpoch 4/5.. Train loss: 1.229.. Test loss: 1.273.. Test accuracy: 0.562\nEpoch 4/5.. Train loss: 1.228.. Test loss: 1.265.. Test accuracy: 0.550\nEpoch 4/5.. Train loss: 1.247.. Test loss: 1.249.. Test accuracy: 0.562\nEpoch 4/5.. Train loss: 1.528.. Test loss: 1.268.. Test accuracy: 0.537\nEpoch 4/5.. Train loss: 1.324.. Test loss: 1.255.. Test accuracy: 0.562\nEpoch 5/5.. Train loss: 1.198.. Test loss: 1.250.. Test accuracy: 0.537\nEpoch 5/5.. Train loss: 1.275.. Test loss: 1.245.. Test accuracy: 0.537\nEpoch 5/5.. Train loss: 1.214.. Test loss: 1.255.. Test accuracy: 0.537\nEpoch 5/5.. Train loss: 1.094.. Test loss: 1.222.. Test accuracy: 0.562\nEpoch 5/5.. Train loss: 1.418.. Test loss: 1.227.. Test accuracy: 0.537\nEpoch 5/5.. Train loss: 1.336.. Test loss: 1.220.. Test accuracy: 0.550\nEpoch 5/5.. Train loss: 1.194.. Test loss: 1.238.. Test accuracy: 0.512\nEpoch 5/5.. Train loss: 1.364.. Test loss: 1.236.. Test accuracy: 0.500\nEpoch 5/5.. Train loss: 1.384.. Test loss: 1.243.. Test accuracy: 0.487\nEpoch 5/5.. Train loss: 1.222.. Test loss: 1.232.. Test accuracy: 0.550\nEpoch 5/5.. Train loss: 1.501.. Test loss: 1.228.. Test accuracy: 0.575\nEpoch 5/5.. Train loss: 1.349.. Test loss: 1.215.. Test accuracy: 0.550\nEpoch 5/5.. Train loss: 1.298.. Test loss: 1.203.. Test accuracy: 0.600\nEpoch 5/5.. Train loss: 1.208.. Test loss: 1.203.. Test accuracy: 0.613\nEpoch 5/5.. Train loss: 1.204.. Test loss: 1.217.. Test accuracy: 0.588\nEpoch 5/5.. Train loss: 1.176.. Test loss: 1.210.. Test accuracy: 0.625\nEpoch 5/5.. Train loss: 1.288.. Test loss: 1.199.. Test accuracy: 0.588\nEpoch 5/5.. Train loss: 1.253.. Test loss: 1.175.. Test accuracy: 0.637\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:35:04.992117Z",
          "start_time": "2021-03-24T09:35:04.987688Z"
        },
        "cell_id": "00008-e806c8c0-1911-4b50-9165-6eb75fb741b5",
        "deepnote_cell_type": "code"
      },
      "source": "list(train_dataset)[0]",
      "outputs": [
        {
          "data": {
            "text/plain": "Dataset ImageFolder\n    Number of datapoints: 434\n    Root location: dataset\n    StandardTransform\nTransform: Compose(\n               Resize(size=[224, 224], interpolation=PIL.Image.BILINEAR)\n               ToTensor()\n               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n           )"
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00009-9de365a9-6cf9-49db-b8c8-46f737acc1df",
        "deepnote_cell_type": "code"
      },
      "source": "\"\"\"for param in model.parameters():\n    param.requires_grad = False\n\n\nclassifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(1024, 500)),\n                          ('relu', nn.ReLU()),\n                          ('fc2', nn.Linear(500, 2)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n    \nmodel.classifier = classifier\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:01:15.449954Z",
          "start_time": "2021-03-24T09:01:15.444593Z"
        },
        "cell_id": "00010-73c9d2bb-ee49-4abe-91b4-c21e052d96c6",
        "deepnote_cell_type": "code"
      },
      "source": "class mask_net(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.model_ft    = models.resnet18(pretrained=True)\n        for param in self.model_ft.parameters():\n            param.requires_grad = False\n        self.num_ftrs    = self.model_ft.fc.in_features\n        classifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(self.num_ftrs, 500)),\n                          ('relu', nn.ReLU()),\n                          ('fc2', nn.Linear(500, 5)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n        self.model_ft.fc = classifier #nn.Linear(self.num_ftrs, 5)\n        \n    def forward(self, x):\n        out = self.model_ft(x)\n        return out\n\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-24T09:13:56.602430Z",
          "start_time": "2021-03-24T09:13:56.096510Z"
        },
        "cell_id": "00011-f4815809-23c7-4a27-882e-b763b0ea339d",
        "deepnote_cell_type": "code"
      },
      "source": "print(models.resnet18(pretrained=True))",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=9f1e7bdb-154a-4549-a646-7775bf1dc7a2' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "deepnote_notebook_id": "9bf6cd2d-3ab6-4f67-b882-e81a30c6e479",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}
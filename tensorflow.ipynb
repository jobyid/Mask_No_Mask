{
  "cells": [
    {
      "cell_type": "code",
      "source": "\nimport tensorflow as tf\nimport numpy as np \nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input",
      "metadata": {
        "tags": [],
        "cell_id": "00000-a94d338d-46f8-40a5-850b-182a8f7473d3",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4c9f94d3",
        "execution_millis": 4,
        "execution_start": 1616510657156,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-bb4482b5-c2a0-4ac5-b1d6-c9b0578ade00",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b35ef045",
        "execution_millis": 6,
        "execution_start": 1616510374368,
        "deepnote_cell_type": "code"
      },
      "source": "train_dir = '/work/Mask_No_Mask/dataset'",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00001-0867e4e9-848d-4da3-a28b-94ea31eef1a7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b623e53d",
        "execution_millis": 3,
        "execution_start": 1616510374378,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n       train_dir,\n        target_size=(224,224),\n        batch_size=16,\n        class_mode='categorical')",
      "metadata": {
        "tags": [],
        "cell_id": "00003-3c0e17cc-6b38-416b-89c7-0d06d20c0c31",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a0a778d5",
        "execution_millis": 116,
        "execution_start": 1616510374821,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Found 434 images belonging to 5 classes.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00002-622bb8ae-5f1f-47e4-b6e3-97b308e2e058",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b623e53d",
        "execution_start": 1616510380403,
        "execution_millis": 3,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00005-025bec79-d8c0-48fe-8684-82d84eb1ea86",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b623e53d",
        "execution_start": 1616510380857,
        "execution_millis": 1,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Multi-Class Image Classification (5 classes)",
      "metadata": {
        "tags": [],
        "cell_id": "00006-d5d05bd7-4c1f-4654-9f40-bf3cb1835ded",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "from tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n\n\n# create the base pre-trained model\nbase_model = tf.keras.applications.ResNet101(weights='imagenet', include_top=False)\n\nx = base_model.output\n\nx = GlobalAveragePooling2D()(x)\n\nx = Dense(1024, activation='relu')(x)\n\n# and a logistic layer -- let's say we have 200 classes\npredictions = Dense(5, activation='softmax')(x)\n\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n\n\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])",
      "metadata": {
        "tags": [],
        "cell_id": "00006-5449a236-0028-4586-9651-92fdaa8633d8",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a777e828",
        "execution_start": 1616510390636,
        "execution_millis": 5295,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# train the model on the new data for a few epochs\nmodel.fit(train_generator, epochs=1)",
      "metadata": {
        "tags": [],
        "cell_id": "00006-dfa1cea6-4c38-4912-8c35-b59f07060059",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c2487a2f",
        "execution_start": 1616510395940,
        "execution_millis": 138370,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "28/28 [==============================] - 138s 5s/step - loss: 2.0514 - acc: 0.2522\n",
          "output_type": "stream"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8a51094150>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "path = '/work/Mask_No_Mask/dataset/design_mask/joby_1616487809.jpg'",
      "metadata": {
        "tags": [],
        "cell_id": "00009-50952692-7f89-4902-b5fe-e0a58ac72831",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e81c325",
        "execution_start": 1616510642074,
        "execution_millis": 2,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "img=image.load_img(path, target_size=(224, 224))\n  \nx=image.img_to_array(img)\nx=np.expand_dims(x, axis=0)\nimages = np.vstack([x])\n\nclasses = model.predict(images, batch_size=1)\npred = classes[0].argmax()\npred",
      "metadata": {
        "tags": [],
        "cell_id": "00005-81b0357b-ce44-40eb-b81b-6ecd0cc02efe",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f1e135e7",
        "execution_millis": 1758,
        "execution_start": 1616510684922,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00010-9647679b-727a-47cc-a171-f28a447b5fbb",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "db8b58f1",
        "execution_millis": 6,
        "execution_start": 1616510722116,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "4"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00012-770558b8-8f79-4db2-ab54-3b03cda1e77d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00012-1ba3d51e-e99a-4de2-bdf2-790e014b02d8",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Multi-Output Class: Classification + Regression ",
      "metadata": {
        "tags": [],
        "cell_id": "00011-ed14f91c-9503-425f-be5d-2f246dc5dade",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "# Load the Resnet Model\n\nimage_size = 224\n\nbackbone = tf.keras.applications.ResNet101( input_tensor = Input(\n    shape=(image_size, image_size, 3)),\n    include_top=False,\n    weights='imagenet'\n    )\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "00012-351310d1-f246-43bb-bdcc-9c090e89d35e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f90d224d",
        "execution_millis": 2793,
        "execution_start": 1616510769210,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#### Loss, Weights, Metrics And Model Compilation",
      "metadata": {
        "tags": [],
        "cell_id": "00013-6183c585-7211-4944-8985-8bca561851b8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "losses = {\n    \"box_output\": \"mean_squared_error\",\n    \"class_output\": \"categorical_crossentropy\"\n    }\n\n\n# Here you can give more or less weightage to each loss.\n\n# If you think that detection is harder then the classification then you can\n\n# Try assinging it more weight\nloss_weights = {\n    \"box_output\": 1.0,\n    \"class_output\": 1.0\n    }\n\n \n\n# For the class labels we want to know the Accuracy\n# And for the bounding boxes we need to know the Mean squared error\nmetrics = {\n    'class_output': 'accuracy',\n    'box_output':  'mse'\n    }\n\n \n# Initialize Optimizer\nopt = tf.keras.optimizers.SGD(lr = 1e-2, momentum = 0.9)\n \n",
      "metadata": {
        "tags": [],
        "cell_id": "00013-2caade7d-1cb7-4cdd-ad71-b57da9688f76",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "3eaee26c",
        "execution_millis": 25,
        "execution_start": 1616511101279,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n\n\n\ndef create_model(no_of_classes):\n\n    # Freeze the whole model\n\n    backbone.trainable = False\n\n    # Start by taking the output feature maps from Resnet\n    base_model_output = backbone.output\n\n     \n\n    # Convert to a single-dimensional vector by Global Average Pooling.\n    flattened_output = GlobalAveragePooling2D()(base_model_output)\n\n \n\n    # Create our Classification Head, final layer contains\n\n    # Ouput units = no. classes\n    class_prediction = Dense(256, activation=\"relu\")(flattened_output)\n    class_prediction = Dense(128, activation=\"relu\")(class_prediction )\n    class_prediction = Dropout(0.2)(class_prediction)\n    class_prediction = Dense(64, activation=\"relu\")(class_prediction)\n    class_prediction = Dropout(0.2)(class_prediction )\n    class_prediction = Dense(32, activation=\"relu\")(class_prediction)\n    class_prediction = Dense(no_of_classes, activation='softmax', name=\"class_output\")(class_prediction)\n\n\n    # Create Our Localization Head, final layer contains 4 nodes for x1,y1,x2,y2\n    # Respectively.\n    box_output = Dense(256, activation=\"relu\")(flattened_output)\n    box_output = Dense(128, activation=\"relu\")(box_output)\n    box_output = Dropout(0.2)(box_output )\n    box_output = Dense(64, activation=\"relu\")(box_output)\n    box_output = Dropout(0.2)(box_output )\n    box_output = Dense(32, activation=\"relu\")(box_output)\n    box_predictions = Dense(5, activation='sigmoid', name= \"box_output\")(box_output)\n\n    # Now combine the two heads\n    model = tf.keras.Model(inputs=backbone.input, outputs= [box_predictions,  class_prediction])\n\n    return model",
      "metadata": {
        "tags": [],
        "cell_id": "00003-ec8e05ba-6037-43d2-a51d-2ab67f920093",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8f86a812",
        "execution_millis": 33,
        "execution_start": 1616511104961,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00016-1174a230-d71c-415e-843d-84e9ff64c3f5",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b623e53d",
        "execution_millis": 8,
        "execution_start": 1616511106566,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "model = create_model(5)",
      "metadata": {
        "tags": [],
        "cell_id": "00004-2eaa458a-98f9-42c0-8afa-da9e38e6bee3",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "889e70d9",
        "execution_millis": 300,
        "execution_start": 1616511107026,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Compile the model with Adam optimizer\nmodel.compile(optimizer = opt, loss = losses, loss_weights = loss_weights,\n    metrics = metrics)",
      "metadata": {
        "tags": [],
        "cell_id": "00018-ea1af602-e410-4249-9d6a-1ec5e3485d9f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b53171d",
        "execution_millis": 81,
        "execution_start": 1616511109963,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# train the model on the new data for a few epochs\nmodel.fit(train_generator, epochs=10)",
      "metadata": {
        "tags": [],
        "cell_id": "00007-54922743-aa61-4a6d-ae25-21fdd33f03ac",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e93885d6",
        "execution_millis": 1888267,
        "execution_start": 1616511112926,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n28/28 [==============================] - 228s 8s/step - loss: 1.9616 - box_output_loss: 0.2382 - class_output_loss: 1.7234 - box_output_mse: 0.2382 - class_output_accuracy: 0.1667\nEpoch 2/10\n28/28 [==============================] - 183s 6s/step - loss: 1.7796 - box_output_loss: 0.1690 - class_output_loss: 1.6107 - box_output_mse: 0.1690 - class_output_accuracy: 0.2842\nEpoch 3/10\n28/28 [==============================] - 172s 6s/step - loss: 1.7640 - box_output_loss: 0.1665 - class_output_loss: 1.5974 - box_output_mse: 0.1665 - class_output_accuracy: 0.2816\nEpoch 4/10\n28/28 [==============================] - 174s 6s/step - loss: 1.7667 - box_output_loss: 0.1616 - class_output_loss: 1.6052 - box_output_mse: 0.1616 - class_output_accuracy: 0.2673\nEpoch 5/10\n28/28 [==============================] - 175s 6s/step - loss: 1.7612 - box_output_loss: 0.1619 - class_output_loss: 1.5992 - box_output_mse: 0.1619 - class_output_accuracy: 0.2737\nEpoch 6/10\n28/28 [==============================] - 178s 7s/step - loss: 1.7527 - box_output_loss: 0.1625 - class_output_loss: 1.5902 - box_output_mse: 0.1625 - class_output_accuracy: 0.3069\nEpoch 7/10\n28/28 [==============================] - 187s 7s/step - loss: 1.7530 - box_output_loss: 0.1631 - class_output_loss: 1.5899 - box_output_mse: 0.1631 - class_output_accuracy: 0.2793\nEpoch 8/10\n28/28 [==============================] - 222s 8s/step - loss: 1.7551 - box_output_loss: 0.1604 - class_output_loss: 1.5947 - box_output_mse: 0.1604 - class_output_accuracy: 0.2864\nEpoch 9/10\n28/28 [==============================] - 172s 6s/step - loss: 1.7470 - box_output_loss: 0.1594 - class_output_loss: 1.5876 - box_output_mse: 0.1594 - class_output_accuracy: 0.2897\nEpoch 10/10\n28/28 [==============================] - 196s 7s/step - loss: 1.7381 - box_output_loss: 0.1595 - class_output_loss: 1.5787 - box_output_mse: 0.1595 - class_output_accuracy: 0.3174\n",
          "output_type": "stream"
        },
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8a1c3a9590>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00016-67137b86-b704-402c-9658-9970972dce38",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00009-887d9985-bcfe-4202-864f-79022933fd1c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "be8e48a0",
        "execution_millis": 57,
        "execution_start": 1616510183562,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00010-1e6c9082-9ff2-474f-955b-b378ceb5689c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b623e53d",
        "execution_millis": 2,
        "execution_start": 1616510185194,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00011-df4b1d37-251e-414c-a4c2-fe465c60a8be",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "00008-9b483665-b7be-4b03-8d20-a107ee146144",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8dc99291",
        "execution_millis": 2745,
        "execution_start": 1616508724698,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00010-ea2ca665-405e-4ea2-922e-48be3494f42c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c2487a2f",
        "execution_millis": 101535,
        "execution_start": 1616509112678,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "55/55 [==============================] - 101s 2s/step - loss: 1.8642 - box_output_loss: 0.1874 - class_output_loss: 1.6768 - box_output_mse: 0.1874 - class_output_accuracy: 0.2185\n",
          "output_type": "stream"
        },
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fdcf37bc750>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00024-1f51621f-ae38-44e0-a697-2e22d593d78e",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from keras.preprocessing import image\nimport numpy as np",
      "metadata": {
        "tags": [],
        "cell_id": "00025-565a1ded-9767-42b4-a8ee-4a91168a600c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a95306e2",
        "execution_millis": 5,
        "execution_start": 1616509993284,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "path = '/work/Mask_No_Mask/dataset/design_mask/joby_1616487809.jpg'",
      "metadata": {
        "tags": [],
        "cell_id": "00026-a150d6da-4d2e-4409-b81c-62e13d8f6b09",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e81c325",
        "execution_millis": 0,
        "execution_start": 1616509993910,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "img=image.load_img(path, target_size=(224, 224))\n  \nx=image.img_to_array(img)\nx=np.expand_dims(x, axis=0)\nimages = np.vstack([x])\n\nclasses = model.predict(images, batch_size=10)",
      "metadata": {
        "tags": [],
        "cell_id": "00026-21204a16-3800-460a-86f8-c486669ed4da",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "374b6826",
        "execution_millis": 3905,
        "execution_start": 1616509994623,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "classes",
      "metadata": {
        "tags": [],
        "cell_id": "00027-7937cb5e-1934-4cf2-982a-ad1fd33057b4",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f8251c81",
        "execution_start": 1616510007061,
        "execution_millis": 13,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 49,
          "data": {
            "text/plain": "[array([[0.1816431 , 0.16929132, 0.1952095 , 0.18487835, 0.36384985]],\n       dtype=float32),\n array([[0.16636616, 0.15433702, 0.22577547, 0.10836096, 0.34516037]],\n       dtype=float32)]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": " print(classes[0])",
      "metadata": {
        "tags": [],
        "cell_id": "00029-13d64a3f-158a-449f-9344-74b2afb72805",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9336456d",
        "execution_start": 1616510031644,
        "execution_millis": 16,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[[0.1816431  0.16929132 0.1952095  0.18487835 0.36384985]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00030-80bbf042-4c02-4457-9867-241420f7900e",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=9f1e7bdb-154a-4549-a646-7775bf1dc7a2' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "4de00530-c1fc-464e-a07e-51eaf75f67ac",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}